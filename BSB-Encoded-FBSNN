import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import time
import matplotlib.pyplot as plt
from abc import ABC, abstractmethod
import matplotlib
import torch.nn.functional as F


def setup_fonts():
    """设置matplotlib字体，避免字体警告"""
    # 首先尝试不使用中文字体
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Helvetica', 'sans-serif']
    plt.rcParams['axes.unicode_minus'] = True  # 使用标准的负号

    # 禁用LaTeX渲染
    plt.rcParams['text.usetex'] = False

    print("使用标准字体设置，避免中文显示问题")


# 调用字体设置函数
setup_fonts()

class Embedding(nn.Module):
    """位置编码模块"""

    def __init__(self, encoding_dim=400):  # 改为400维
        super().__init__()
        self.encoding_dim = encoding_dim

    def forward(self, t):
        batch_size = t.shape[0]
        half_dim = self.encoding_dim // 2

        # 创建位置索引
        positions = t

        # 计算频率项
        i = torch.arange(0, half_dim, dtype=torch.float32, device=t.device)
        inv_freq = 1.0 / (10000.0 ** (2.0 * i / self.encoding_dim))

        # 计算位置编码
        pos_enc = torch.zeros(batch_size, self.encoding_dim, device=t.device)

        # 偶数位置使用sin
        pos_enc[:, 0::2] = torch.sin(positions * inv_freq.view(1, -1))
        # 奇数位置使用cos
        pos_enc[:, 1::2] = torch.cos(positions * inv_freq.view(1, -1))

        # 将原始坐标值复制encoding_dim次
        t_repeated = t.repeat(1, self.encoding_dim)

        # 相加
        return t_repeated + pos_enc


class XReshapeEncoder(nn.Module):
    """100维x编码器：先reshape为10×10，然后插值为20×20"""

    def __init__(self, input_dim=100, initial_size=(10, 10), target_size=(20, 20)):
        super().__init__()
        self.input_dim = input_dim
        self.initial_size = initial_size
        self.target_size = target_size

        # 验证维度匹配
        assert initial_size[0] * initial_size[1] == input_dim, \
            f"初始图像尺寸必须满足{initial_size[0]}×{initial_size[1]} = {input_dim}"

    def forward(self, x, return_features=True):
        """
        输入: x [batch_size, 100]
        输出: image [batch_size, 1, 20, 20], features [batch_size, 100]
        """
        batch_size = x.shape[0]

        # 1. 先reshape为10×10图像
        x_10x10 = x.view(batch_size, 1,
                         self.initial_size[0],
                         self.initial_size[1])  # [batch_size, 1, 10, 10]

        # 2. 使用双线性插值上采样到20×20
        x_20x20 = F.interpolate(x_10x10, size=self.target_size,
                                mode='bilinear', align_corners=False)  # [batch_size, 1, 20, 20]

        if return_features:
            # 直接返回原始特征
            return x_20x20, x
        return x_20x20


class CoordinateToImage(nn.Module):
    """时间编码器：编码成400维然后转换成20×20"""

    def __init__(self, encoding_dim=400, image_size=(20, 20)):
        super().__init__()
        self.encoding_dim = encoding_dim
        self.image_size = image_size
        self.encoder = Embedding(encoding_dim)

    def forward(self, x, return_features=True):
        encoded = self.encoder(x)  # [batch_size, encoding_dim=400]
        batch_size = x.shape[0]

        # 将400维编码reshape为20×20图像
        image = encoded.view(batch_size, 1,
                             self.image_size[0],
                             self.image_size[1])  # [batch_size, 1, 20, 20]

        if return_features:
            # 返回编码后的特征
            return image, encoded
        return image


class CNN(nn.Module):
    """CNN结构20×20输入图像"""

    def __init__(self):
        super(CNN, self).__init__()

        # 第一层卷积：输入2通道，输出64通道，20×20 -> 10×10 (经过MaxPool2d)
        self.conv1 = nn.Sequential(
            nn.Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=1),  # 20×20 -> 20×20
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)  # 20×20 -> 10×10
        )

        # 第二层卷积：64通道 -> 128通道，10×10 -> 2×2 (经过AdaptiveAvgPool2d)
        self.conv2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=1),  # 10×10 -> 10×10
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)  # 10×10 -> 2×2
        )

        # 全连接层：128 * 2 * 2 = 512维 -> 256维 -> 1维
        self.fc = nn.Sequential(
            nn.Linear(128 * 5 * 5, 256),  # 512维 -> 256维
            nn.ReLU(),
            nn.Linear(256, 1)  # 256维 -> 1维
        )

    def forward(self, x):
        x = self.conv1(x)  # [batch_size, 64, 10, 10]
        x = self.conv2(x)  # [batch_size, 128, 2, 2]
        x = x.reshape(x.size(0), -1)  # [batch_size, 128*2*2=512]
        x = self.fc(x)  # [batch_size, 1]
        return x


class PositionalEncodingCNN(nn.Module):
    """CNN处理20×20输入图像"""

    def __init__(self, input_dim=101, encoding_dim=400, initial_image_size=(10, 10),
                 target_image_size=(20, 20), output_dim=1):
        """
        输入: [batch_size, 101] (1维时间 + 100维空间)
        输出: [batch_size, 1]
        """
        super().__init__()

        # 对100维x使用reshape+插值编码器
        self.x_to_image = XReshapeEncoder(
            input_dim=100,  # 100维空间
            initial_size=initial_image_size,  # 先reshape为10×10
            target_size=target_image_size  # 然后插值为20×20
        )

        # t编码器：编码成400维然后转换为20×20
        self.t_to_image = CoordinateToImage(
            encoding_dim=encoding_dim,  # 400维
            image_size=target_image_size  # 20×20
        )

        # 使用修改后的CNN，适配20×20输入
        self.cnn = CNN()  # CNN输出1维

    def forward(self, x):
        """
        输入: x [batch_size, 101] (1维时间 + 100维空间)
        """
        # 分离时间和空间 - 第一维是时间，后面是空间
        t_coord = x[:, 0:1]  # 第一维是时间坐标
        x_coord = x[:, 1:101]  # 后面100维是空间坐标

        # 分别对t和x进行编码和图像化
        t_image, _ = self.t_to_image(t_coord, return_features=True)  # [batch_size, 1, 20, 20]
        x_image, _ = self.x_to_image(x_coord, return_features=True)  # [batch_size, 1, 20, 20]

        # 合并x和t的特征图
        combined_image = torch.cat([t_image, x_image], dim=1)  # [batch_size, 2, 20, 20]

        # 通过CNN得到输出
        output = self.cnn(combined_image)  # [batch_size, 1]
        return output


class FBSNN(ABC):  # Forward-Backward Stochastic Neural Network
    def __init__(self, Xi, T, M, N, D, layers, total_trajectories=5000, test_trajectories=1000, epsilon=0.0,
                 device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.Xi = torch.tensor(Xi, dtype=torch.float32, device=device, requires_grad=False)  # 初始点
        self.T = T  # 终端时间

        self.M = M  # 每次训练的轨迹数量（批次大小）
        self.N = N  # 时间快照数量
        self.D = D  # 维度
        self.total_trajectories = total_trajectories  # 训练轨道数量
        self.test_trajectories = test_trajectories  # 测试轨道数量
        self.total_all_trajectories = total_trajectories + test_trajectories  # 总轨道数

        self.layers = layers  # 神经网络层结构
        self.device = device
        self.epsilon = epsilon  # 因果权重系数

        # 初始化固定轨道集
        self.fixed_t = None
        self.fixed_W = None
        self.generate_all_trajectories()  # 生成所有轨道（训练+测试）

        # 初始化神经网络 - 使用修改后的神经网络架构
        self.model = PositionalEncodingCNN(
            input_dim=D + 1,  # D维空间 + 1维时间
            encoding_dim=400,  # 时间编码维度改为400
            initial_image_size=(10, 10),  # x初始reshape尺寸
            target_image_size=(20, 20),  # 最终图像尺寸
            output_dim=1
        ).to(device)

        # 优化器：优化所有模型参数
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0)

        # 记录损失
        self.loss_history = []
        self.loss_components_history = []  # 记录损失各组成部分
        self.min_weight_history = []  # 记录最小权重历史
        self.Y0_history = []  # 记录每次迭代的Y0值
        self.YT_history = []  # 记录每次迭代的YT值
        self.avg_weight_history = []  # 记录平均权重历史
        self.epsilon_history = []  # 记录每次迭代的epsilon值



    def generate_all_trajectories(self):
        """生成所有的轨道集，包括训练集和测试集，只在初始化时运行一次"""
        T = self.T
        total, N, D = self.total_all_trajectories, self.N, self.D
        dt = T / N

        # 设置随机种子以确保每次运行生成相同的轨道
        torch.manual_seed(42)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(42)

        # 时间增量和布朗运动增量
        Dt = torch.zeros(total, N + 1, 1, device=self.device)
        Dt[:, 1:, :] = dt

        DW = torch.zeros(total, N + 1, D, device=self.device)
        DW[:, 1:, :] = np.sqrt(dt) * torch.randn(total, N, D, device=self.device)

        # 累积得到时间和布朗运动路径
        self.fixed_t = torch.cumsum(Dt, dim=1)  # total x (N+1) x 1
        self.fixed_W = torch.cumsum(DW, dim=1)  # total x (N+1) x D
        print(
            f"已生成 {self.total_all_trajectories} 条固定轨道（{self.total_trajectories}条训练 + {self.test_trajectories}条测试），将用于所有训练迭代")

    def get_batch_from_fixed(self):
        """从训练轨道集中随机采样一个批次"""
        # 随机选择M个索引（仅从训练集中选择）
        indices = torch.randperm(self.total_trajectories, device=self.device)[:self.M]
        # 选择对应的轨道
        batch_t = self.fixed_t[indices]
        batch_W = self.fixed_W[indices]
        return batch_t, batch_W

    def get_test_trajectories(self, num_trajectories=None):
        """获取测试轨道集"""
        if num_trajectories is None:
            num_trajectories = self.test_trajectories

        # 从测试集中获取轨道（索引从total_trajectories开始）
        indices = torch.arange(self.total_trajectories,
                               self.total_trajectories + num_trajectories,
                               device=self.device)
        # 选择对应的轨道
        test_t = self.fixed_t[indices]
        test_W = self.fixed_W[indices]
        return test_t, test_W

    def get_train_trajectories(self, num_trajectories):
        """获取指定数量的训练轨道"""
        if num_trajectories > self.total_trajectories:
            num_trajectories = self.total_trajectories

        # 从训练集中获取轨道
        indices = torch.arange(0, num_trajectories, device=self.device)
        train_t = self.fixed_t[indices]
        train_W = self.fixed_W[indices]
        return train_t, train_W

    def initialize_NN(self, layers):
        """初始化神经网络"""
        return PositionalEncodingCNN(
            input_dim=self.D + 1,
            encoding_dim=400,
            initial_image_size=(10, 10),
            target_image_size=(20, 20),
            output_dim=1
        )

    def neural_net(self, X):
        """神经网络前向传播"""
        return self.model(X)

    def net_u(self, t, X, requires_grad=True):  # t: M x 1, X: M x D
        """计算u和其对X的梯度"""
        if requires_grad:
            # 训练阶段：启用梯度计算
            X = X.clone().requires_grad_(True)
            t = t.clone().requires_grad_(True)

            # 合并输入
            input_tensor = torch.cat([t, X], dim=1)
            u = self.neural_net(input_tensor)

            # 计算梯度
            Du = torch.autograd.grad(u.sum(), X, create_graph=True, retain_graph=True)[0]  # M x D
        else:
            # 预测阶段：简化计算，不计算精确梯度，实际无需计算梯度
            with torch.no_grad():
                # 合并输入
                input_tensor = torch.cat([t, X], dim=1)
                u = self.neural_net(input_tensor)

                # 简化：返回近似零梯度（如果需要梯度可启用有限差分）
                Du = torch.zeros_like(X)

                if False:  # 设置为False以禁用梯度计算
                    # 启用梯度进行单次计算
                    X_grad = X.clone().requires_grad_(True)
                    t_grad = t.clone().requires_grad_(True)
                    input_grad = torch.cat([t_grad, X_grad], dim=1)
                    u_grad = self.neural_net(input_grad)
                    Du = torch.autograd.grad(u_grad.sum(), X_grad, retain_graph=False)[0]
                    Du = Du.detach()

        return u, Du

    def Dg(self, X, requires_grad=True):  # X: M x D
        """计算g对X的梯度"""
        batch_size = X.shape[0]  # 获取实际批次大小
        if requires_grad:
            # 训练阶段：启用梯度计算
            X = X.clone().requires_grad_(True)
            g = self.g(X)
            Dg = torch.autograd.grad(g.sum(), X, create_graph=True, retain_graph=True)[0]  # M x D
        else:
            # 预测阶段：简化处理
            with torch.no_grad():
                # 计算g值
                g_val = self.g(X)
                # 对于预测，返回零梯度（或近似）
                Dg = torch.zeros_like(X)

                # 如果需要精确梯度，可以启用有限差分（但效率低）
                if False:  # 设置为False以提高效率
                    epsilon = 1e-4
                    Dg = torch.zeros_like(X)
                    for d in range(X.shape[1]):
                        X_plus = X.clone()
                        X_minus = X.clone()
                        X_plus[:, d] += epsilon
                        X_minus[:, d] -= epsilon

                        g_plus = self.g(X_plus)
                        g_minus = self.g(X_minus)

                        Dg[:, d] = (g_plus - g_minus).squeeze() / (2 * epsilon)
        return Dg

    def loss_function(self, t, W, Xi, verbose=False):  # t: M x (N+1) x 1, W: M x (N+1) x D, Xi: 1 x D
        loss = 0.0
        X_list = []
        Y_list = []

        # 获取当前批次的实际大小（兼容训练和预测）
        batch_size = t.shape[0]

        # 存储每个轨迹在每个时间步的BSDE损失 (batch_size x N)
        temporal_losses_per_path = torch.zeros(batch_size, self.N, device=self.device)

        # 记录各部分的损失
        bsde_loss = 0.0  # BSDE内部一致性损失
        terminal_loss = 0.0  # 终端条件损失
        gradient_loss = 0.0  # 梯度匹配损失

        # 初始时刻
        t0 = t[:, 0, :]  # batch_size x 1
        W0 = W[:, 0, :]  # batch_size x D
        X0 = Xi.repeat(batch_size, 1)  # batch_size x D
        Y0, Z0 = self.net_u(t0, X0, requires_grad=True)  # batch_size x 1, batch_size x D

        X_list.append(X0)
        Y_list.append(Y0)

        # 时间迭代
        for n in range(self.N):
            t1 = t[:, n + 1, :]  # batch_size x 1
            W1 = W[:, n + 1, :]  # batch_size x D

            # 计算X的下一步
            mu = self.mu(t0, X0, Y0, Z0)  # batch_size x D
            sigma = self.sigma(t0, X0, Y0)  # batch_size x D x D

            # 计算随机积分项
            dW = W1 - W0  # batch_size x D
            sigma_dW = torch.bmm(sigma, dW.unsqueeze(-1)).squeeze(-1)  # batch_size x D

            X1 = X0 + mu * (t1 - t0) + sigma_dW  # batch_size x D

            # 计算Y的预测值
            phi = self.phi(t0, X0, Y0, Z0)  # batch_size x 1
            Y1_tilde = Y0 + phi * (t1 - t0) + torch.sum(Z0 * sigma_dW, dim=1, keepdim=True)  # batch_size x 1

            # 神经网络预测的Y1和Z1
            Y1, Z1 = self.net_u(t1, X1, requires_grad=True)  # batch_size x 1, batch_size x D

            # 计算每个轨迹在当前时间步的BSDE损失 (batch_size,)
            bsde_step_loss_per_path = torch.sum((Y1 - Y1_tilde) ** 2, dim=1)  # batch_size
            temporal_losses_per_path[:, n] = bsde_step_loss_per_path

            # 计算当前时间步的总BSDE损失（用于记录）
            bsde_step_loss = torch.sum(bsde_step_loss_per_path)
            bsde_loss += bsde_step_loss

            # 更新变量，准备下一步迭代
            t0, W0, X0, Y0, Z0 = t1, W1, X1, Y1, Z1

            X_list.append(X0)
            Y_list.append(Y0)

        # 计算每个时间步的所有轨道平均损失
        temporal_losses_per_timestep = torch.sum(temporal_losses_per_path, dim=0)  # (N,)

        # 计算每个时间步的未来损失累积和（从后往前累积）
        future_loss_cumsum = torch.zeros(self.N, device=self.device)

        # 从最后一个时间步开始向前累积
        future_loss_cumsum[-1] = 0.0  # 最后一个时间步没有未来损失
        for n in range(self.N - 2, -1, -1):
            # n 的未来损失 = (n+1 的未来损失) + (n+1 的损失)
            future_loss_cumsum[n] = future_loss_cumsum[n + 1] + temporal_losses_per_timestep[n + 1]

        # 冻结梯度以避免影响权重计算
        with torch.no_grad():
            future_loss_cumsum_frozen = future_loss_cumsum.detach().clone()

        # 计算每个时间步的权重：exp(-epsilon * 未来损失之和)
        weights_per_timestep = torch.exp(-self.epsilon * future_loss_cumsum_frozen)

        # 扩展为每条轨道的权重 (batch_size x N)
        weights_per_path = weights_per_timestep.unsqueeze(0).repeat(batch_size, 1)  # batch_size x N

        # 计算加权BSDE损失
        weighted_bsde_loss = torch.sum(weights_per_path * temporal_losses_per_path)

        # 计算统计信息
        min_weight = torch.min(weights_per_timestep).item()
        avg_weight = torch.mean(weights_per_timestep).item()

        # 计算未加权的总BSDE损失（用于分析）
        unweighted_bsde_loss = torch.sum(temporal_losses_per_path).item()

        # 终端条件损失
        g_X1 = self.g(X1)  # batch_size x 1
        terminal_loss = torch.sum((Y1 - g_X1) ** 2)

        # 梯度匹配损失
        Dg_X1 = self.Dg(X1, requires_grad=True)  # batch_size x D
        gradient_loss = torch.sum((Z1 - Dg_X1) ** 2)

        # 总损失 = 加权BSDE损失 + 终端损失 + 梯度损失
        total_loss = weighted_bsde_loss + terminal_loss + gradient_loss

        # 堆叠结果
        X = torch.stack(X_list, dim=1)  # batch_size x (N+1) x D
        Y = torch.stack(Y_list, dim=1)  # batch_size x (N+1) x 1

        # 打印详细的损失信息
        if verbose:
            print(f"=== Loss Components Breakdown ===")
            print(f"BSDE Loss (加权):       {weighted_bsde_loss.item():.3e}")
            print(f"BSDE Loss (未加权):     {unweighted_bsde_loss:.3e}")
            print(f"Terminal Condition Loss: {terminal_loss.item():.3e}")
            print(f"Gradient Matching Loss:  {gradient_loss.item():.3e}")
            print(f"Total Loss:              {total_loss.item():.3e}")
            print(f"BSDE Ratio:              {(weighted_bsde_loss.item() / total_loss.item() * 100):.1f}%")
            print(f"Terminal Ratio:          {(terminal_loss.item() / total_loss.item() * 100):.1f}%")
            print(f"Gradient Ratio:          {(gradient_loss.item() / total_loss.item() * 100):.1f}%")
            print(f"Min Weight (时间步):     {min_weight:.3e}")
            print(f"Avg Weight (时间步):     {avg_weight:.3e}")
            print(f"Y0 Prediction:           {Y[0, 0, 0].item():.6f}")
            print(f"YT Prediction:           {Y[0, -1, 0].item():.6f}")
            print(f"g(XT) True Value:        {g_X1[0, 0].item():.6f}")
            print(f"Current epsilon:         {self.epsilon:.6f}")

        return total_loss, X, Y, Y[0, 0, 0], (
            weighted_bsde_loss.item(), terminal_loss.item(), gradient_loss.item()), min_weight, avg_weight

    def fetch_minibatch(self):
        """使用固定轨道"""
        return self.get_batch_from_fixed()

    # 分段训练方法，支持同时指定迭代次数、学习率和epsilon
    def train_segment(self, num_iterations, learning_rate, epsilon=None, verbose_freq=50, save_freq=1000):
        """单段训练，可指定迭代次数、学习率和epsilon"""
        # 更新学习率
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = learning_rate

        # 更新epsilon（如果提供）
        if epsilon is not None:
            self.epsilon = epsilon

        start_time = time.time()
        for it in range(num_iterations):
            # 测量数据获取时间
            data_start = time.time()
            # 从固定轨道集中获取批次数据
            t_batch, W_batch = self.get_batch_from_fixed()
            data_time = time.time() - data_start

            # 前向传播计算损失
            self.optimizer.zero_grad()

            # 只在特定迭代次数详细打印损失信息
            verbose = (it % verbose_freq == 0) or (it == num_iterations - 1)

            # 测量前向传播时间
            forward_start = time.time()
            loss, _, _, Y0_pred, loss_components, min_weight, avg_weight = self.loss_function(t_batch, W_batch, self.Xi,
                                                                                              verbose=verbose)
            forward_time = time.time() - forward_start

            # 测量反向传播时间
            backward_start = time.time()
            loss.backward()
            self.optimizer.step()
            backward_time = time.time() - backward_start

            # 记录损失和当前epsilon
            self.loss_history.append(loss.item())
            self.loss_components_history.append(loss_components)
            self.min_weight_history.append(min_weight)
            self.Y0_history.append(Y0_pred.item())
            self.YT_history.append(loss_components[1])  # 记录终端损失
            self.avg_weight_history.append(avg_weight)
            self.epsilon_history.append(self.epsilon)  # 记录当前epsilon值

            # 打印信息
            if it % 10 == 0:
                elapsed = time.time() - start_time
                bsde_loss, terminal_loss, gradient_loss = loss_components
                print(f'It: {len(self.loss_history):3d}, Loss: {loss.item():.3e}, '
                      f'Y0: {Y0_pred.item():.3f}, Epsilon: {self.epsilon:.3f}, '
                      f'MinW: {min_weight:.2e}, AvgW: {avg_weight:.2f}, '
                      f'Data: {data_time * 1000:.1f}ms, '
                      f'Forward: {forward_time * 1000:.1f}ms, '
                      f'Backward: {backward_time * 1000:.1f}ms, '
                      f'Total: {elapsed:.2f}s')
                start_time = time.time()

    def get_single_fixed_trajectory(self, trajectory_idx=0, from_test_set=False):
        """从固定轨道集中获取单条轨道"""
        if from_test_set:
            # 从测试集中获取
            if trajectory_idx < 0 or trajectory_idx >= self.test_trajectories:
                raise ValueError(f"测试轨道索引应在[0, {self.test_trajectories - 1}]范围内")
            actual_idx = self.total_trajectories + trajectory_idx
        else:
            # 从训练集中获取
            if trajectory_idx < 0 or trajectory_idx >= self.total_trajectories:
                raise ValueError(f"训练轨道索引应在[0, {self.total_trajectories - 1}]范围内")
            actual_idx = trajectory_idx

        # 选择单条轨道
        batch_t = self.fixed_t[actual_idx:actual_idx + 1]  # 保持batch维度
        batch_W = self.fixed_W[actual_idx:actual_idx + 1]

        source = "测试集" if from_test_set else "训练集"
        print(f"从{source}中选择第 {trajectory_idx} 条轨道进行预测 (总索引: {actual_idx})")
        return batch_t.cpu().numpy(), batch_W.cpu().numpy()

    def predict(self, Xi_star, t_star, W_star):
        """预测结果"""
        if isinstance(Xi_star, torch.Tensor):
            Xi_star = Xi_star.clone().detach().to(self.device, dtype=torch.float32)
        else:
            Xi_star = torch.tensor(Xi_star, dtype=torch.float32, device=self.device)

        if isinstance(t_star, torch.Tensor):
            t_star = t_star.clone().detach().to(self.device, dtype=torch.float32)
        else:
            t_star = torch.tensor(t_star, dtype=torch.float32, device=self.device)

        if isinstance(W_star, torch.Tensor):
            W_star = W_star.clone().detach().to(self.device, dtype=torch.float32)
        else:
            W_star = torch.tensor(W_star, dtype=torch.float32, device=self.device)

        # 预测
        with torch.no_grad():
            loss = 0.0
            X_list = []
            Y_list = []

            # 获取当前预测的批次大小
            batch_size = t_star.shape[0]

            # 初始时刻
            t0 = t_star[:, 0, :]  # batch_size x 1
            W0 = W_star[:, 0, :]  # batch_size x D
            X0 = Xi_star.repeat(batch_size, 1)  # batch_size x D (使用实际批次大小)
            Y0, Z0 = self.net_u(t0, X0, requires_grad=False)  # 预测阶段不计算梯度

            X_list.append(X0)
            Y_list.append(Y0)

            # 时间迭代
            for n in range(self.N):
                t1 = t_star[:, n + 1, :]  # batch_size x 1
                W1 = W_star[:, n + 1, :]  # batch_size x D

                # 计算X的下一步
                mu = self.mu(t0, X0, Y0, Z0)  # batch_size x D
                sigma = self.sigma(t0, X0, Y0)  # batch_size x D x D

                # 计算随机积分项
                dW = W1 - W0  # batch_size x D
                sigma_dW = torch.bmm(sigma, dW.unsqueeze(-1)).squeeze(-1)  # batch_size x D

                X1 = X0 + mu * (t1 - t0) + sigma_dW  # batch_size x D

                # 计算Y的预测值
                phi = self.phi(t0, X0, Y0, Z0)  # batch_size x 1
                Y1_tilde = Y0 + phi * (t1 - t0) + torch.sum(Z0 * sigma_dW, dim=1, keepdim=True)  # batch_size x 1

                # 神经网络预测的Y1和Z1
                Y1, Z1 = self.net_u(t1, X1, requires_grad=False)  # 预测阶段不计算梯度

                # 更新变量，准备下一步迭代
                t0, W0, X0, Y0, Z0 = t1, W1, X1, Y1, Z1

                X_list.append(X0)
                Y_list.append(Y0)

            # 堆叠结果
            X_pred = torch.stack(X_list, dim=1)  # batch_size x (N+1) x D
            Y_pred = torch.stack(Y_list, dim=1)  # batch_size x (N+1) x 1

        return X_pred.cpu().numpy(), Y_pred.cpu().numpy()


    @abstractmethod
    def phi(self, t, X, Y, Z):  # M x 1, M x D, M x 1, M x D
        pass  # 返回 M x 1

    @abstractmethod
    def g(self, X):  # M x D
        pass  # 返回 M x 1

    @abstractmethod
    def mu(self, t, X, Y, Z):  # M x 1, M x D, M x 1, M x D
        batch_size = X.shape[0]
        return torch.zeros(batch_size, self.D, device=self.device)  # batch_size x D

    @abstractmethod
    def sigma(self, t, X, Y):  # M x 1, M x D, M x 1
        batch_size = X.shape[0]
        return torch.diag_embed(torch.ones(batch_size, self.D, device=self.device))  # batch_size x D x D



# =============  Black-Scholes-Barenblatt 方程 =============
class BlackScholesBarenblatt(FBSNN):
    def __init__(self, Xi, T, M, N, D, layers, total_trajectories=5000, test_trajectories=1000, epsilon=0.0,
                 device='cuda' if torch.cuda.is_available() else 'cpu'):
        super().__init__(Xi, T, M, N, D, layers, total_trajectories, test_trajectories, epsilon, device)

    def phi(self, t, X, Y, Z):  # batch_size x 1, batch_size x D, batch_size x 1, batch_size x D
        # 0.05*(Y - sum(X*Z))
        return 0.05 * (Y - torch.sum(X * Z, dim=1, keepdim=True))  # batch_size x 1

    def g(self, X):  # batch_size x D
        # sum(X^2)
        return torch.sum(X ** 2, dim=1, keepdim=True)  # batch_size x 1

    def mu(self, t, X, Y, Z):  # batch_size x 1, batch_size x D, batch_size x 1, batch_size x D
        # 零漂移项
        batch_size = X.shape[0]
        return torch.zeros(batch_size, self.D, device=self.device)  # batch_size x D

    def sigma(self, t, X, Y):  # batch_size x 1, batch_size x D, batch_size x 1
        # 0.4 * diag(X)
        batch_size = X.shape[0]
        return 0.4 * torch.diag_embed(X)  # batch_size x D x D


def plot_loss_components(loss_components_history):
    """绘制损失函数各组成部分的变化，新增epsilon变化曲线"""
    if not loss_components_history:
        return

    iterations = range(len(loss_components_history))
    bsde_losses = [comp[0] for comp in loss_components_history]
    terminal_losses = [comp[1] for comp in loss_components_history]
    gradient_losses = [comp[2] for comp in loss_components_history]

    plt.figure(figsize=(12, 6))  # 增大图的尺寸

    plt.rcParams['xtick.labelsize'] = 16  # x轴刻度标签字体大小
    plt.rcParams['ytick.labelsize'] = 16  # y轴刻度标签字体大小

    # 绘制各损失分量
    plt.subplot(1, 2, 1)
    plt.semilogy(iterations, bsde_losses, 'b-', label='BSDE Loss', alpha=0.7)
    plt.semilogy(iterations, terminal_losses, 'r-', label='Terminal Loss', alpha=0.7)
    plt.semilogy(iterations, gradient_losses, 'g-', label='Gradient Loss', alpha=0.7)
    plt.xlabel('Iteration', fontsize=18)
    plt.ylabel('Loss Value', fontsize=18)
    # plt.title('100-dimensional Black-Scholes-Barenblatt(CNN)')
    plt.legend(fontsize=14)
    plt.grid(True, alpha=0.3)


    # 绘制总损失
    plt.subplot(1, 2, 2)
    total_losses = [bsde + terminal + gradient for bsde, terminal, gradient in
                    zip(bsde_losses, terminal_losses, gradient_losses)]
    plt.semilogy(iterations, total_losses, 'k-', linewidth=2, label='Total Loss')
    plt.xlabel('Iteration', fontsize=18)
    plt.ylabel('Total Loss', fontsize=18)
    # plt.title('Total Training Loss(CNN)')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()


def plot_prediction_comparison(t, Y_pred, Y_exact, title="", trajectory_idx=0, from_test_set=False):
    """绘制预测值与精确解的对比图"""
    source = "测试集" if from_test_set else "训练集"

    plt.rcParams['xtick.labelsize'] = 26  # x轴刻度标签字体大小
    plt.rcParams['ytick.labelsize'] = 26  # y轴刻度标签字体大小

    plt.figure(figsize=(12, 8))
    plt.plot(t[0, :, 0], Y_pred[0, :, 0], 'b-', linewidth=2.5, label='Predicted $u(t,X_t)$')
    plt.plot(t[0, :, 0], Y_exact[:, 0], 'r--', linewidth=2.5, label='Exact $u(t,X_t)$')

    terminal_g = Y_exact[-1, 0]
    plt.plot(t[0, -1, 0], terminal_g, 'ks', markersize=12, label='$Y_T = u(T,X_T)$')
    plt.plot([0], Y_exact[0, 0], 'ko', markersize=12, label='$Y_0 = u(0,X_0)$')

    plt.xlabel('$t$', fontsize=28)
    plt.ylabel('$Y_t = u(t,X_t)$', fontsize=28)
    # plt.title(f'{title}-{source}', fontsize=16)
    plt.legend(fontsize=22, loc='best')
    plt.grid(True, alpha=0.3)

    plt.show()

    return np.mean(relative_error), np.max(relative_error)


def compute_exact_solution(t, X, T=1.0):
    """计算 Black-Scholes-Barenblatt 方程的精确解"""
    # 精确解公式: u(t,X) = exp((r + sigma_max^2)*(T - t)) * sum(X^2)
    r = 0.05
    sigma_max = 0.4

    # t 的形状为 (N+1, 1)，X 的形状为 (N+1, D)
    # 计算每个时间点的精确解
    time_factor = np.exp((r + sigma_max ** 2) * (T - t.squeeze()))  # (N+1,)
    sum_X_squared = np.sum(X ** 2, axis=1)  # (N+1,)

    # 返回 (N+1, 1) 形状
    return (time_factor * sum_X_squared)[:, np.newaxis]


if __name__ == "__main__":
    # 配置参数
    M = 100  # 每次训练的轨迹数量（批次大小）
    total_trajectories = 5000  # 训练轨道总数
    test_trajectories = 1000  # 测试轨道总数增加到1000条
    N = 50  # 时间快照数量
    D = 100  # 维度
    initial_epsilon = 0.1  # 初始因果权重系数

    # 神经网络结构（注意：此处记录原神经网络结构，实际使用的是新架构）
    layers = [D + 1] + 4 * [256] + [1]

    # 初始点：对于 Black-Scholes-Barenblatt 方程，使用 [1.0, 0.5] 重复 D/2 次
    Xi = np.array([1.0, 0.5] * (D // 2))[None, :]
    T = 1.0

    # 选择设备
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")
    print(f"求解方程: Black-Scholes-Barenblatt 方程")
    print(f"维度 D: {D}")
    print(f"初始点 Xi: {Xi[0, :4]}... (重复{D // 2}次)")
    print(f"终端时间 T: {T}")
    print(f"初始因果权重系数 (epsilon): {initial_epsilon}")
    print(f"权重设置方式: 按时间步设置 (每个时间步一个权重)")
    print(f"训练轨道总数: {total_trajectories} 条")
    print(f"测试轨道总数: {test_trajectories} 条")
    print(f"总轨道数: {total_trajectories + test_trajectories} 条")
    print(f"每次训练批次大小: {M}")
    print(f"神经网络架构: PositionalEncodingCNN (改进版)")
    print(f"  - 时间编码: 1维 -> 400维 -> 20×20图像")
    print(f"  - 空间编码: 100维 -> reshape为10×10 -> 双线性插值为20×20")
    print(f"  - 合并输入: 2通道20×20图像")

    # 创建模型
    model = BlackScholesBarenblatt(Xi, T, M, N, D, layers, total_trajectories, test_trajectories, initial_epsilon,
                                   device)

    print("Starting training...")
    print(
        f"Configuration: M={M}, total_trajectories={total_trajectories}, test_trajectories={test_trajectories}, N={N}, D={D}")
    print(f"Network architecture: Improved PositionalEncodingCNN with CNN")
    print(f"  - Input: 101-dim (1 time + 100 space)")
    print(f"  - Time encoding: 1 -> 400 -> 20×20")
    print(f"  - Space encoding: 100 -> 10×10 -> bilinear interpolation -> 20×20")
    print(f"  - Combined: 2-channel 20×20 image -> CNN -> 1 output")
    print(f"Initial condition: {Xi[0, :4]}...")
    print(f"Terminal time: {T}")
    print("-" * 80)

    # 分阶段训练，支持同时调节学习率和epsilon
    model.train_segment(num_iterations=2000, learning_rate=1e-3, epsilon=0.0, verbose_freq=50, save_freq=1000)
    model.train_segment(num_iterations=1000, learning_rate=1e-4, epsilon=0.0, verbose_freq=50, save_freq=1000)

    # 绘制损失分量分析图
    if hasattr(model, 'loss_components_history') and model.loss_components_history:
        plot_loss_components(model.loss_components_history)

    print("\n" + "=" * 80)
    print("开始预测阶段...")
    print("=" * 80)

    # ==================== 第一部分：从训练集中选取所有5000条轨道进行预测 ====================
    print("\n1. 从训练集中选取所有5000条轨道进行预测...")
    print("-" * 60)

    # 获取训练集中的所有5000条轨道
    train_t, train_W = model.get_train_trajectories(num_trajectories=total_trajectories)
    train_t_np = train_t.cpu().numpy()
    train_W_np = train_W.cpu().numpy()

    # 对训练集的所有5000条轨道进行预测
    print(f"正在对{total_trajectories}条训练轨道进行预测...")
    X_pred_train_all, Y_pred_train_all = model.predict(model.Xi, train_t_np, train_W_np)

    # 计算每条轨道的相对误差
    print("计算训练轨道相对误差...")
    train_errors = []
    for i in range(total_trajectories):
        # 计算精确解
        Y_exact = compute_exact_solution(train_t_np[i, :, :], X_pred_train_all[i, :, :], T)

        # 计算相对误差
        relative_error = np.abs((Y_pred_train_all[i, :, 0] - Y_exact[:, 0]) / (np.abs(Y_exact[:, 0]) + 1e-8))
        avg_error = np.mean(relative_error)
        max_error = np.max(relative_error)

        train_errors.append((avg_error, max_error))

        # 每500条轨道打印一次进度
        if (i + 1) % 500 == 0:
            print(f"已处理 {i + 1}/{total_trajectories} 条训练轨道")

    # 计算训练集的平均误差
    train_avg_errors = np.array([err[0] for err in train_errors])
    train_max_errors = np.array([err[1] for err in train_errors])

    # 输出训练集统计信息
    print(f"\n训练集{total_trajectories}条轨道统计:")
    print(f"平均相对误差 (均值±标准差): {np.mean(train_avg_errors) * 100:.2f}% ± {np.std(train_avg_errors) * 100:.2f}%")
    print(f"最大相对误差 (均值±标准差): {np.mean(train_max_errors) * 100:.2f}% ± {np.std(train_max_errors) * 100:.2f}%")
    print(f"平均相对误差范围: [{np.min(train_avg_errors) * 100:.2f}%, {np.max(train_avg_errors) * 100:.2f}%]")
    print(f"最大相对误差范围: [{np.min(train_max_errors) * 100:.2f}%, {np.max(train_max_errors) * 100:.2f}%]")

    # 从训练集中选择一条轨道绘制详细图（选择误差中位数的轨道）
    selected_train_idx = np.argsort(train_avg_errors)[len(train_avg_errors)//2]  # 选择误差中位数的轨道
    print(
        f"\n选择训练轨道 {selected_train_idx} 进行详细可视化 (平均误差: {train_avg_errors[selected_train_idx] * 100:.2f}%)")

    # 获取单条训练轨道
    train_t_single_np, train_W_single_np = model.get_single_fixed_trajectory(
        trajectory_idx=selected_train_idx, from_test_set=False
    )

    # 预测单条训练轨道
    X_pred_train, Y_pred_train = model.predict(model.Xi, train_t_single_np, train_W_single_np)

    # 计算精确解
    Y_exact_train = compute_exact_solution(train_t_single_np[0, :, :], X_pred_train[0, :, :], T)


    # 绘制训练轨道的对比图
    plot_prediction_comparison(train_t_single_np, Y_pred_train, Y_exact_train,
                               title="100维 Black-Scholes-Barenblatt 方程预测结果",
                               trajectory_idx=selected_train_idx,
                               from_test_set=False)

    # ==================== 第二部分：从测试集中选取所有1000条轨道进行预测 ====================
    print("\n\n2. 从测试集中选取所有1000条轨道进行预测...")
    print("-" * 60)

    # 获取测试集中的所有1000条轨道
    test_t, test_W = model.get_test_trajectories()
    test_t_np = test_t.cpu().numpy()
    test_W_np = test_W.cpu().numpy()

    # 对测试集的所有1000条轨道进行预测
    print(f"正在对{test_trajectories}条测试轨道进行预测...")
    X_pred_test_all, Y_pred_test_all = model.predict(model.Xi, test_t_np, test_W_np)

    # 计算每条测试轨道的相对误差
    print("计算测试轨道相对误差...")
    test_errors = []
    for i in range(test_trajectories):
        # 计算精确解
        Y_exact = compute_exact_solution(test_t_np[i, :, :], X_pred_test_all[i, :, :], T)

        # 计算相对误差
        relative_error = np.abs((Y_pred_test_all[i, :, 0] - Y_exact[:, 0]) / (np.abs(Y_exact[:, 0]) + 1e-8))
        avg_error = np.mean(relative_error)
        max_error = np.max(relative_error)

        test_errors.append((avg_error, max_error))

        # 每100条轨道打印一次进度
        if (i + 1) % 100 == 0:
            print(f"已处理 {i + 1}/{test_trajectories} 条测试轨道")

    # 计算测试集的平均误差
    test_avg_errors = np.array([err[0] for err in test_errors])
    test_max_errors = np.array([err[1] for err in test_errors])

    # 输出测试集统计信息
    print(f"\n测试集{test_trajectories}条轨道统计:")
    print(f"平均相对误差 (均值±标准差): {np.mean(test_avg_errors) * 100:.2f}% ± {np.std(test_avg_errors) * 100:.2f}%")
    print(f"最大相对误差 (均值±标准差): {np.mean(test_max_errors) * 100:.2f}% ± {np.std(test_max_errors) * 100:.2f}%")
    print(f"平均相对误差范围: [{np.min(test_avg_errors) * 100:.2f}%, {np.max(test_avg_errors) * 100:.2f}%]")
    print(f"最大相对误差范围: [{np.min(test_max_errors) * 100:.2f}%, {np.max(test_max_errors) * 100:.2f}%]")

    # 从测试集中选择一条轨道绘制详细图（选择误差中位数的轨道）
    selected_test_idx = np.argsort(test_avg_errors)[len(test_avg_errors)//2]  # 选择误差中位数的轨道
    print(
        f"\n选择测试轨道 {selected_test_idx} 进行详细可视化 (平均误差: {test_avg_errors[selected_test_idx] * 100:.2f}%)")

    # 获取单条测试轨道
    test_t_single_np, test_W_single_np = model.get_single_fixed_trajectory(
        trajectory_idx=selected_test_idx, from_test_set=True
    )

    # 预测单条测试轨道
    X_pred_test, Y_pred_test = model.predict(model.Xi, test_t_single_np, test_W_single_np)

    # 计算精确解
    Y_exact_test = compute_exact_solution(test_t_single_np[0, :, :], X_pred_test[0, :, :], T)

    # 绘制测试轨道的对比图
    plot_prediction_comparison(test_t_single_np, Y_pred_test, Y_exact_test,
                               title="100维 Black-Scholes-Barenblatt 方程预测结果",
                               trajectory_idx=selected_test_idx,
                               from_test_set=True)

    # ==================== 最终统计信息 ====================
    print("\n" + "=" * 80)
    print("最终统计信息")
    print("=" * 80)
    print(f"求解方程: Black-Scholes-Barenblatt 方程")
    print(f"神经网络架构: 改进的PositionalEncodingCNN")
    print(f"  - 时间编码维度: 400维")
    print(f"  - 时间图像尺寸: 20×20")
    print(f"  - 空间处理: 100维 -> 10×10 -> 双线性插值 -> 20×20")
    print(f"  - 输入图像: 2通道20×20")
    print(f"训练轨道数量: {total_trajectories} 条")
    print(f"测试轨道数量: {test_trajectories} 条")
    print(f"总轨道数量: {total_trajectories + test_trajectories} 条")
    print(f"训练迭代次数: {len(model.loss_history)}")
    print(f"最终训练损失: {model.loss_history[-1]:.6f}" if model.loss_history else "N/A")
    print(f"最终 Epsilon: {model.epsilon:.6f}")

    print(f"\n训练集误差统计 ({total_trajectories}条轨道):")
    print(f"平均相对误差范围: [{np.min(train_avg_errors) * 100:.2f}%, {np.max(train_avg_errors) * 100:.2f}%]")
    print(f"平均相对误差: {np.mean(train_avg_errors) * 100:.2f}% ± {np.std(train_avg_errors) * 100:.2f}%")

    print(f"\n测试集误差统计 ({test_trajectories}条轨道):")
    print(f"平均相对误差范围: [{np.min(test_avg_errors) * 100:.2f}%, {np.max(test_avg_errors) * 100:.2f}%]")
    print(f"平均相对误差: {np.mean(test_avg_errors) * 100:.2f}% ± {np.std(test_avg_errors) * 100:.2f}%")

    print(f"\n误差对比:")
    print(f"训练集 vs 测试集平均误差: {np.mean(train_avg_errors) * 100:.2f}% vs {np.mean(test_avg_errors) * 100:.2f}%")
    print(f"差异: {(np.mean(test_avg_errors) - np.mean(train_avg_errors)) * 100:.2f}%")

    print(f"\n轨道选择说明:")
    print(f"- 训练集: 使用全部{total_trajectories}条训练轨道进行预测")
    print(f"- 测试集: 使用全部{test_trajectories}条测试轨道进行预测")
    print(
        f"- 可视化: 从每个集合中选择误差中位数的轨道进行可视化 (训练轨道 {selected_train_idx}, 测试轨道 {selected_test_idx})")
